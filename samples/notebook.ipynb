{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff4565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfbb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANIMATION_TIME_SCALE = 1\n",
    "\n",
    "files = ['mbpp_samples.json', 'humaneval_samples.json', 'gsm8k_cot_samples.json', 'minerva_math_samples.json']\n",
    "\n",
    "all_streaming_data_neo_entry = []\n",
    "for id, file in enumerate(files):\n",
    "    with open(f'./{file}', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    tidar_steps =data['models']['tidar_8b']['steps']\n",
    "\n",
    "    # create a new json file, that contains a list of decoded tidar steps\n",
    "    already_decoded_tokens = []\n",
    "    decoding_progress = []\n",
    "    cum_tokens = 0\n",
    "    for i in range(len(tidar_steps)):\n",
    "        already_accepted_tokens = tokenizer.decode(already_decoded_tokens, skip_special_tokens=True)\n",
    "        already_decoded_tokens.extend(tidar_steps[i][0][:tidar_steps[i][1]])\n",
    "        draft_tokens = tokenizer.decode(tidar_steps[i][0], skip_special_tokens=True)\n",
    "        \n",
    "        decoding_progress.append((already_accepted_tokens, draft_tokens, cum_tokens))\n",
    "        cum_tokens += tidar_steps[i][1]\n",
    "\n",
    "    speculation_data = {}\n",
    "\n",
    "    speculation_data['name'] = 'TiDAR-8B'\n",
    "    speculation_data['link'] = 'https://arxiv.org/pdf/2511.08923'\n",
    "    speculation_data['speculative_progress'] = decoding_progress\n",
    "    speculation_data['response'] = decoding_progress[-1][0] + decoding_progress[-1][1]\n",
    "    speculation_data['actual_stream_time'] = data['models']['tidar_8b']['decode_latency'] / 512 \n",
    "    speculation_data['animation_time'] = speculation_data['actual_stream_time'] * ANIMATION_TIME_SCALE\n",
    "    speculation_data['total_tokens'] = cum_tokens\n",
    "\n",
    "    streaming_data_neo_entry = {}\n",
    "    streaming_data_neo_entry['id'] = id\n",
    "\n",
    "    if file == 'gsm8k_cot_samples.json':\n",
    "        streaming_data_neo_entry['question'] = \"[GSM-8K] Q:\" + data['prompt'].split(\"Q:\")[-1]\n",
    "        streaming_data_neo_entry['actual_prompt'] = \"[GSM-8K] ... (8-shot prompt) ...\\nQ:\" + data['prompt'].split(\"Q:\")[-1]\n",
    "    elif file == 'mbpp_samples.json':\n",
    "        streaming_data_neo_entry['question'] = \"[MBPP] You are an expert Python programmer\" + data['prompt'].split(\"You are an expert Python programmer\")[-1]\n",
    "        streaming_data_neo_entry['actual_prompt'] = \"[MBPP]... (3-shot prompt) ...\\nYou are an expert Python programmer\" + data['prompt'].split(\"You are an expert Python programmer\")[-1]\n",
    "    elif file == 'minerva_math_samples.json':\n",
    "        streaming_data_neo_entry['question'] = \"[Minerva Math] Problem:\" + data['prompt'].split(\"Problem:\")[-1]\n",
    "        streaming_data_neo_entry['actual_prompt'] = \"[Minerva Math] ... (4-shot prompt) ...\\nProblem:\" + data['prompt'].split(\"Problem:\")[-1]\n",
    "    else:\n",
    "        streaming_data_neo_entry['question'] = \"[HumanEval] \" + data['prompt']\n",
    "        streaming_data_neo_entry['actual_prompt'] = \"[HumanEval]\" + data['prompt']\n",
    "    \n",
    "    streaming_data_neo_entry['model_a'] = speculation_data\n",
    "    streaming_data_neo_entry['model_b'] = {\n",
    "        'name': 'Qwen3-8B-Base',\n",
    "        'link': 'https://huggingface.co/Qwen/Qwen3-8B-Base',\n",
    "        'response': data['models']['qwen3_8b']['output'],\n",
    "        'total_tokens': speculation_data['total_tokens'],\n",
    "        'actual_stream_time': data['models']['qwen3_8b']['decode_latency'] / 512 ,\n",
    "        'animation_time': data['models']['qwen3_8b']['decode_latency'] / 512 * ANIMATION_TIME_SCALE,\n",
    "    }\n",
    "    all_streaming_data_neo_entry.append(streaming_data_neo_entry)\n",
    "\n",
    "with open('../streaming_data_neo.json', 'w') as f:\n",
    "    json.dump(all_streaming_data_neo_entry, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aeac92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mr_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
